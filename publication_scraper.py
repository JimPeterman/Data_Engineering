import requests, bs4from bs4 import BeautifulSoup as bsfrom fake_useragent import UserAgentimport pickle# Use random user agent when scraping.# (and need a user id to scrape BSU website).ua = UserAgent()user_agent = {'User-agent': ua.random}url = "https://www.bsu.edu/academics/centersandinstitutes/wellness/research/research-reports-and-publications/friend-reports"response = requests.get(url, headers = user_agent)status = response.status_codeif status == 200:    page = response.text    soup = bs(page)else:    print(f"Oops! Received status code {status}")       # Find the table of papers.    table = soup.find('div', attrs = {'class': 'col-md-9 col-md-push-3 main-content'})# Create a list of the publications formatted to post with link.pubs_lst = []for paper in table.find_all("li"):        citation = paper.text    citation = citation.replace("\xa0", " ")    citation = citation.replace("\n", "")    # In case there's no webpage associated with the paper.    try:        web = paper.find("a")["href"]    except:        web = ""            if len(web) > 1:        pubs_lst.append(f"[{citation}]({web})")    else:        pubs_lst.append(f"{citation}")# Save the list. with open('./data/publications.pickle', 'wb') as to_write:    pickle.dump(pubs_lst, to_write)